{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2:  the perceptron\n",
    "\n",
    "\n",
    "* **By Vignesh M. Pagadala**\n",
    "* **Vignesh.Pagadala@colostate.edu**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Datasets\n",
    "\n",
    "In this assignment we will use the following datasets:\n",
    "  * The [Gisette](http://archive.ics.uci.edu/ml/datasets/Gisette) handwritten digit recognition dataset. \n",
    "  * The [QSAR](http://archive.ics.uci.edu/ml/datasets/QSAR+biodegradation) data for predicting the biochemical activity of a molecule.\n",
    "  * The [Heart disease diagnosis](http://archive.ics.uci.edu/ml/datasets/Heart+Disease) dataset.\n",
    "  * For developing your code, you can use one of the scikit-learn datasets, such as the [breast cancer wisconsin dataset](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html#sklearn.datasets.load_breast_cancer) and the [make_classification](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html#sklearn.datasets.make_classification) toy dataset generator.\n",
    "  \n",
    "When writing your notebook, you can assume the datasets are in the same directory as the notebook.  Please keep the same file names as in the UCI repository.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1:  Variants of the perceptron algorithm \n",
    "\n",
    "In this assignment you will work with several variants of the perceptron algorithm:\n",
    "\n",
    "  * The \"vanila\" version of the perceptron algorithm, which was introduced in class.\n",
    "  * The pocket algorithm as described in the slides or page 80 in the book.\n",
    "  * The **adatron** version of the perceptron described next.\n",
    "\n",
    "In each case make sure that your implementation of the classifier **includes a bias term** (in slide set 2 and page 7 in the book you will find guidance on how to add a bias term to an algorithm that is expressed without one).\n",
    "\n",
    "## The adatron \n",
    "\n",
    "Before we get to the adatron, we will derive an alternative form of the perceptron algorithm --- the dual perceptron algorithm.  All we need to look at is the weight update rule:\n",
    "\n",
    "$$\\mathbf{w} \\rightarrow \\mathbf{w} + \\eta y_i \\mathbf{x}_i.$$\n",
    "\n",
    "This is performed whenever example $i$ is misclassified by the current weight vector.  The thing to notice, is that the weight vector is always a weighted combination of the training examples since it is that way to begin with, and each update maintains that property.  So in fact, rather than representing $\\mathbf{w}$ explicitly, all we need to do is to keep track of how much each training example is contributing to the value of the weight vector, i.e. we will express it as:\n",
    "\n",
    "$$\\mathbf{w} = \\sum_{i=1}^N \\alpha_i y_i \\mathbf{x}_i,$$\n",
    "\n",
    "where $\\alpha_i$ are positive numbers that describe the magnitude of the contribution $\\mathbf{x}_i$ is making to the weight vector, and $N$ is the number of training examples.\n",
    "\n",
    "Therefore to initialize $\\mathbf{w}$ to 0, we simply initialize $\\alpha_i = 0$ for $i = 1,\\ldots,N$.  When expressed using the variables $\\alpha_i$, the perceptron update rule becomes:\n",
    "\n",
    "$$\\alpha_i = \\alpha_i + \\eta y_i,$$\n",
    "\n",
    "and you can always retrieve the weight vector using its expansion in terms of the $\\alpha_i$.\n",
    "\n",
    "Now we're ready for the adatron - the only difference is in the initialization and update equation.\n",
    "\n",
    "Initialization:\n",
    "\n",
    "$\\alpha_i = 1$ for $i = 1,\\ldots,N$\n",
    "\n",
    "Like in the perceptron we run the algorithm until convergence, or until a fixed number of epochs has passed (an epoch is a loop over all the training data), and an epoch of training consists of the following procedure:\n",
    "\n",
    "for each training example $i=1,\\ldots,N$ perform the following steps:\n",
    "\n",
    "1.  $\\gamma = y_i * \\mathbf{w}^{t} \\mathbf{x}_i$\n",
    "2.  $\\delta\\alpha = \\eta * (1 - \\gamma)$\n",
    "3.  `if` $(\\alpha_i + \\delta\\alpha < 0)$ : $\\alpha_i = 0$, `else : ` $\\alpha_i = \\alpha_i + \\delta\\alpha$\n",
    "\n",
    "\n",
    "The variable $\\eta$ plays the role of the learning rate $\\eta$ employed in the perceptron algorithm and $\\delta \\alpha$ is the proposed magnitude of change in $\\alpha_i$. \n",
    "We note that the adatron tries to maintain a **sparse** representation in terms of the training examples by keeping many $\\alpha_i$ equal to zero.  The adatron converges to a special case of the SVM algorithm that we will learn later in the semester; this algorithm tries to maximize the margin with which each example is classified, which is captured by the variable $\\gamma$ in the algorithm (notice that the magnitude of change proposed for each $\\alpha_i$ becomes smaller as $\\gamma$ increases towards 1).\n",
    "\n",
    "**Note:** if you observe an overflow issues in running the adatron, add an upper bound on the value of $\\alpha_i$.\n",
    "\n",
    "Here's what you need to do:\n",
    "\n",
    "  - Implement the pocket algorithm and the adatron; each classifier should be implemented in a separate Python class, and use the same interface used in the code provided for the perceptron algorithm, i.e. provides the same methods with the same signature.  Make sure each classifier you use (including the original version of the perceptron) implements a bias term.\n",
    "  - Compare the performance of these variants of the perceptron on the Gisette and QSAR datasets by computing an estimate of the out of sample error on a sample of the data that you reserve for testing (the test set).  In each case reserve about 60% of the data for training, and 40% for testing.  To gain more confidence in our error estimates, repeat this experiment using 10 random splits of the data into training/test sets.  Report the average error and its standard deviation in a nicely formatted table.  Is there a version of the perceptron that appears to perform better?   (In answering this, consider the differences in performance you observe in comparison to the standard deviation).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANSWER: #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import statistics\n",
    "from astropy.table import Table, Column\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.a. Perceptron - Vanilla Version##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implements the perceptron algorithm, including a bias term.\n",
    "class Perceptron :\n",
    "    def __init__(self, max_iterations=100, learning_rate=0.2, bias = 0) :\n",
    "        self.max_iterations = max_iterations\n",
    "        self.learning_rate = learning_rate\n",
    "        self.bias = bias\n",
    " \n",
    "    def fit(self, X, y) :\n",
    "        self.w = np.zeros(len(X[0]))\n",
    "        # ADDING BIAS TERM\n",
    "        # Change input to accomodate bias. Tack a column of 1s.\n",
    "        X = np.insert(X, 0, 1, 1)\n",
    "        # Include bias in weight\n",
    "        self.w = np.insert(self.w, 0, self.bias)\n",
    "        converged = False\n",
    "        iterations = 0\n",
    "        #Iterate through all datasets, and update weights based on misclassified examples.\n",
    "        while (not converged and iterations <= self.max_iterations) :\n",
    "            converged = True\n",
    "            for i in range(len(X)) :\n",
    "                if y[i] * self.decision_function(X[i]) <= 0 :\n",
    "                    self.w = self.w + y[i] * self.learning_rate * X[i]\n",
    "                    converged = False\n",
    "            iterations += 1\n",
    "        self.converged = converged\n",
    "        if converged :\n",
    "            pass\n",
    "        return self.w\n",
    " \n",
    "    def decision_function(self, x) :\n",
    "        return np.inner(self.w, x)\n",
    " \n",
    "    def predict(self, X) :\n",
    "        scores = np.inner(self.w, X)\n",
    "        return np.sign(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.b. Perceptron - Pocket Algorithm##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pocket :\n",
    "    def __init__(self, max_iterations=100, learning_rate=0.2, bias = 0) :\n",
    "        self.max_iterations = max_iterations\n",
    "        self.learning_rate = learning_rate\n",
    "        self.bias = bias\n",
    "\n",
    "    def fit(self, X, y) :\n",
    "        self.w = np.zeros(len(X[0]))\n",
    "        # Initializing pocket to zero.\n",
    "        self.w_pocket = np.zeros(len(X[0]))\n",
    "        # ADDING BIAS TERM\n",
    "        # Change input to accomodate bias. Tack a column of 1s.\n",
    "        X = np.insert(X, 0, 1, 1)\n",
    "        # Include bias in weights\n",
    "        self.w = np.insert(self.w, 0, self.bias)\n",
    "        self.w_pocket = np.insert(self.w_pocket, 0, self.bias)\n",
    "        converged = False\n",
    "        iterations = 0\n",
    "        while (not converged and iterations <= self.max_iterations) :\n",
    "            converged = True\n",
    "            for i in range(len(X)) :\n",
    "                if y[i] * self.decision_function(X[i]) <= 0 : # if a misclassified example is detected\n",
    "                    self.w = self.w + y[i] * self.learning_rate * X[i]\n",
    "                    # Calculate Ein with Wpocket\n",
    "                    EinPocket = self.inSample(X, y, self.w_pocket)\n",
    "                    # Calculate Ein with W\n",
    "                    Ein = self.inSample(X, y, self.w)\n",
    "                    # If EinW lesser than EinPocket, then Wpocket = W\n",
    "                    if Ein < EinPocket:\n",
    "                        self.w_pocket = self.w\n",
    "                    converged = False\n",
    "            iterations += 1\n",
    "        self.converged = converged\n",
    "        if converged :\n",
    "            pass\n",
    "        return self.w_pocket\n",
    "    \n",
    "    # Utility function to calculate the in-sample error. Find number of misclassified examples, and divide by total.\n",
    "    def inSample(self, X, y, w):\n",
    "        misclass = 0\n",
    "        for i in range(len(X)):\n",
    "            if y[i] * np.inner(w, X[i]) <= 0:\n",
    "                misclass += 1\n",
    "        Ein = misclass / len(X)\n",
    "        return Ein\n",
    "\n",
    "    def decision_function(self, x) :\n",
    "        return np.inner(self.w, x)\n",
    "\n",
    "    def predict(self, X) :\n",
    "        scores = np.inner(self.w, X)\n",
    "        return np.sign(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.c. Perceptron - Adatron Version##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adatron :\n",
    "    def __init__(self, max_iterations=100, learning_rate=0.2, bias = 0) :\n",
    "        self.max_iterations = max_iterations\n",
    "        self.learning_rate = learning_rate\n",
    "        self.bias = bias\n",
    "\n",
    "    def fit(self, X, y) :\n",
    "        alpha = np.ones(len(X))\n",
    "        # Calculate weight\n",
    "        self.w = np.zeros(len(X[0]))\n",
    "        # ADDING BIAS TERM\n",
    "        # Change input to accomodate bias. Tack a column of 1s.\n",
    "        X = np.insert(X, 0, 1, 1)\n",
    "        # Include bias in weight\n",
    "        self.w = np.insert(self.w, 0, self.bias)\n",
    "        # Compute initial weight from summation.\n",
    "        for i in range(len(X)):\n",
    "            self.w = self.w + alpha[i] * y[i] * X[i]\n",
    "        iterations = 0\n",
    "        while (iterations <= self.max_iterations) :\n",
    "            for i in range(len(X)) :\n",
    "                # Update alpha/weight for every example.\n",
    "                oldalpha = alpha\n",
    "                gamma = y[i] * np.inner(self.w, X[i])\n",
    "                da = self.learning_rate * (1 - gamma)\n",
    "                if (alpha[i] + da) < 0:\n",
    "                    alpha[i] = 0\n",
    "                else:\n",
    "                    alpha[i] = alpha[i] + da\n",
    "                # UPDATE WEIGHT - remove previous value from weight, and add newly computed weight. More efficient,\n",
    "                # than doing the summation again.\n",
    "                #old weight\n",
    "                oldw = self.w\n",
    "                # new weight\n",
    "                neww = (oldw - (oldalpha[i] * y[i] * X[i])) + alpha[i] * y[i] * X[i]\n",
    "                self.w = neww\n",
    "            iterations += 1\n",
    "        return self.w\n",
    "\n",
    "    def decision_function(self, x) :\n",
    "        return np.inner(self.w, x)\n",
    "    \n",
    "    # Function to calculate in sample error.\n",
    "    def inSample(self, X, y, w):\n",
    "        misclass = 0\n",
    "        for i in range(len(X)):\n",
    "            if y[i] * np.inner(w, X[i]) <= 0:\n",
    "                misclass += 1\n",
    "        Ein = misclass / len(X)\n",
    "        return Ein\n",
    "\n",
    "    def predict(self, X) :\n",
    "        scores = np.inner(self.w, X)\n",
    "        return np.sign(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Comparision##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Gisette Dataset ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine train and validation sets for input and target.\n",
    "train_data = np.genfromtxt(\"gisette_train.data\", delimiter = \" \")\n",
    "valid_data = np.genfromtxt(\"gisette_valid.data\", delimiter = \" \")\n",
    "X = np.concatenate((train_data, valid_data), axis = 0)\n",
    "train_labels = np.genfromtxt(\"gisette_train.labels\", delimiter = \" \")\n",
    "valid_labels = np.genfromtxt(\"gisette_valid.labels\", delimiter = \" \")\n",
    "y = np.concatenate((train_labels, valid_labels), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate Out of Sample Error\n",
    "def outSample(obj, Xtrain, ytrain, Xtest, ytest, bias = 0):\n",
    "    # Calculate weight vector using function.\n",
    "    w = obj.fit(Xtrain, ytrain)\n",
    "    \n",
    "    # Use this weight vector to compute Eout. Inlcude bias term here.\n",
    "    # Change input to accomodate bias. Tack a column of 1s.\n",
    "    # Number of examples misclassified divided by total (out of sample) in the test data, when using weight vector from fitting the training data.\n",
    "    Xtest = np.insert(Xtest, 0, 1, 1)\n",
    "    misclass = 0\n",
    "    for i in range(len(Xtest)):\n",
    "        if ytest[i] * np.inner(w, Xtest[i]) <= 0:\n",
    "            misclass += 1\n",
    "    Eout = misclass / len(Xtest)\n",
    "    return Eout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists used to store out of sample error for every iteration of randomized testing.\n",
    "PerceptronEGisette = [] # Out of sample error when using Perceptron on Gisette.\n",
    "PocketEGisette = [] # Pocket - Gisette\n",
    "AdatronEGisette = [] # Adatron - Gisette\n",
    "\n",
    "# Create objects\n",
    "p1 = Perceptron()\n",
    "p2 = Pocket()\n",
    "p3 = Adatron()\n",
    "\n",
    "# Repeat 10 times.\n",
    "for i in range(9):\n",
    "    # Split into training and testing datasets (60 - 40).\n",
    "    nRows = X.shape[0]\n",
    "    nTrain = int(round(0.6*nRows)) \n",
    "    nTest = nRows - nTrain\n",
    "    # Shuffle row numbers to randomize the train and test data.\n",
    "    rows = np.arange(nRows)\n",
    "    np.random.shuffle(rows)\n",
    "    trainIndices = rows[:nTrain]\n",
    "    testIndices = rows[nTrain:]\n",
    "    \n",
    "    # Obtain input - train, input - test, output - train and output - test data.\n",
    "    Xtrain = X[trainIndices, :]\n",
    "    ytrain = y[trainIndices]\n",
    "    Xtest = X[testIndices, :]\n",
    "    ytest = y[testIndices]\n",
    "    \n",
    "    # Get Eout for all three classifiers and append to list.\n",
    "    PerceptronEGisette.append(outSample(p1, Xtrain, ytrain, Xtest, ytest))\n",
    "    PocketEGisette.append(outSample(p2, Xtrain, ytrain, Xtest, ytest))\n",
    "    AdatronEGisette.append(outSample(p3, Xtrain, ytrain, Xtest, ytest))\n",
    "    \n",
    "PerceptronEGisetteMean = sum(PerceptronEGisette)/len(PerceptronEGisette)\n",
    "PerceptronEGisetteSD = statistics.stdev(PerceptronEGisette)\n",
    "PocketEGisetteMean = sum(PocketEGisette)/len(PocketEGisette)\n",
    "PocketEGisetteSD = statistics.stdev(PocketEGisette)\n",
    "AdatronEGisetteMean = sum(AdatronEGisette)/len(AdatronEGisette)\n",
    "AdatronEGisetteSD = statistics.stdev(AdatronEGisette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of Sample       Perceptron              Pocket              Adatron      \n",
      "------------- --------------------- --------------------- -------------------\n",
      "         Mean   0.03305555555555555   0.03305555555555555 0.24730158730158733\n",
      "           SD 0.0035495360752084704 0.0035495360752084704 0.08518458183180458\n"
     ]
    }
   ],
   "source": [
    "# This code is for creating the table.\n",
    "a = [PerceptronEGisetteMean, PerceptronEGisetteSD]\n",
    "b = [PocketEGisetteMean, PocketEGisetteSD]\n",
    "c = [AdatronEGisetteMean, AdatronEGisetteSD]\n",
    "e = ['Mean', 'SD']\n",
    "qt = Table([e, a, b, c], names=('Out of Sample', 'Perceptron', 'Pocket', 'Adatron'))\n",
    "print(qt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above case, it appears that the lowest mean out-of-sample error is achieved by the Perceptron (Vanilla) and the Pocket variants (whose error averages appear to be the same for this dataset). Adatron has clearly performed the worst with the highest value of error. There also appears to be a lot of variation in the error values in the Adatron classifier i.e. the error values are more spread out than the errors for the Pocket and Perceptron algorithms. This indicates that the **Perceptron and Pocket have achieved a lesser error value more consistently that the Adatron**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. QSAR Dataset ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1055, 42)\n"
     ]
    }
   ],
   "source": [
    "# Load the csv data.\n",
    "dframe=pd.read_csv('biodeg.csv', sep=';', header=None)\n",
    "print(dframe.shape)\n",
    "\n",
    "# Get target.\n",
    "Td = dframe.iloc[:, -1]\n",
    "Td = Td.as_matrix()\n",
    "Td[Td == 'RB'] = 1\n",
    "Td[Td == 'NRB'] = -1\n",
    "y = Td\n",
    "# Get input.\n",
    "Xd = dframe.iloc[:, : -1]\n",
    "Xd = Xd.as_matrix()\n",
    "X = Xd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists for storing error values for the three classifiers.\n",
    "PerceptronEQSAR = []\n",
    "PocketEQSAR = []\n",
    "AdatronEQSAR = []\n",
    "\n",
    "p1 = Perceptron()\n",
    "p2 = Pocket()\n",
    "p3 = Adatron()\n",
    "\n",
    "for i in range(9):\n",
    "    # Split into training (60 %) and testing data (40 %).\n",
    "    nRows = X.shape[0]\n",
    "    nTrain = int(round(0.6*nRows)) \n",
    "    nTest = nRows - nTrain\n",
    "    # Shuffle row numbers\n",
    "    rows = np.arange(nRows)\n",
    "    np.random.shuffle(rows)\n",
    "    trainIndices = rows[:nTrain]\n",
    "    testIndices = rows[nTrain:]\n",
    "    Xtrain = X[trainIndices, :]\n",
    "    ytrain = y[trainIndices]\n",
    "    Xtest = X[testIndices, :]\n",
    "    ytest = y[testIndices]\n",
    "    \n",
    "    # Get Eout for all three classifiers.\n",
    "    PerceptronEQSAR.append(outSample(p1, Xtrain, ytrain, Xtest, ytest))\n",
    "    PocketEQSAR.append(outSample(p2, Xtrain, ytrain, Xtest, ytest))\n",
    "    AdatronEQSAR.append(outSample(p3, Xtrain, ytrain, Xtest, ytest))\n",
    "    \n",
    "PerceptronEQSARMean = sum(PerceptronEQSAR)/len(PerceptronEQSAR)\n",
    "PerceptronEQSARSD = statistics.stdev(PerceptronEQSAR)\n",
    "PocketEQSARMean = sum(PocketEQSAR)/len(PocketEQSAR)\n",
    "PocketEQSARSD = statistics.stdev(PocketEQSAR)\n",
    "AdatronEQSARMean = sum(AdatronEQSAR)/len(AdatronEQSAR)\n",
    "AdatronEQSARSD = statistics.stdev(AdatronEQSAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of Sample      Perceptron            Pocket             Adatron       \n",
      "------------- ------------------- ------------------- --------------------\n",
      "         Mean  0.2335439705107952 0.15034228541337546    0.325434439178515\n",
      "           SD 0.08248957024045925 0.01709248420883286 0.018621129911731878\n"
     ]
    }
   ],
   "source": [
    "# Printing a table.\n",
    "a = [PerceptronEQSARMean, PerceptronEQSARSD]\n",
    "b = [PocketEQSARMean, PocketEQSARSD]\n",
    "c = [AdatronEQSARMean, AdatronEQSARSD]\n",
    "e = ['Mean', 'SD']\n",
    "qt = Table([e, a, b, c], names=('Out of Sample', 'Perceptron', 'Pocket', 'Adatron'))\n",
    "print(qt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even in the case of the QSAR dataset, Adatron has performed the worst, with an average out-of-sample error value around 0.3. However, in this case, the Perceptron and Pocket algorithms have yeilded better results, with the latter performing better that the perceptron. Also, in terms of the standard deviation, the Pocket algorithm looks more promising, due to the fact that it has scored (across the 10 rounds) less error more consistently, since it has the lowest standard deviation value.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, this leads me to conclude that the Pocket algorithms seems to be doing much better than the other variants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2:  Learning Curves \n",
    "\n",
    "Whenever we train a classifier it is useful to know if we have collected a sufficient amount of data for accurate classification.  A good way of determining that is to construct a **learning curve**, which is a plot of classifier performance (i.e. its error) as a function of the number of training examples.  Plot a learning curve for the perceptron algorithm (with bias) using the Gisette dataset.  The x-axis for the plot (number of training examples) should be on a logarithmic scale - something like 10,20,40,80,200,400,800.  Use numbers that are appropriate for the dataset at hand, choosing values that illustrate the variation that you observe.  What can you conclude from the learning curve you have constructed?  Make sure that you use a fixed test set to evaluate performance while varying the size of the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANSWER:#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine train and validation sets.\n",
    "train_data = np.genfromtxt(\"gisette_train.data\", delimiter = \" \")\n",
    "valid_data = np.genfromtxt(\"gisette_valid.data\", delimiter = \" \")\n",
    "X = np.concatenate((train_data, valid_data), axis = 0)\n",
    "train_labels = np.genfromtxt(\"gisette_train.labels\", delimiter = \" \")\n",
    "valid_labels = np.genfromtxt(\"gisette_valid.labels\", delimiter = \" \")\n",
    "y = np.concatenate((train_labels, valid_labels), axis = 0)\n",
    "\n",
    "# Split into training and testing datasets (80 - 20).\n",
    "nRows = X.shape[0]\n",
    "nTrain = int(round(0.8*nRows)) \n",
    "nTest = nRows - nTrain\n",
    "# Shuffle row numbers\n",
    "rows = np.arange(nRows)\n",
    "np.random.shuffle(rows)\n",
    "trainIndices = rows[:nTrain]\n",
    "testIndices = rows[nTrain:]\n",
    "\n",
    "Xtrain = X[trainIndices, :]\n",
    "ytrain = y[trainIndices]\n",
    "Xtest = X[testIndices, :]\n",
    "ytest = y[testIndices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5600, 5000)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Store the out sample error in a numpy array\n",
    "Xtrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.26785714285714285,\n",
       " 0.17285714285714285,\n",
       " 0.17285714285714285,\n",
       " 0.16071428571428573,\n",
       " 0.11857142857142858,\n",
       " 0.07214285714285715,\n",
       " 0.05785714285714286,\n",
       " 0.047857142857142855,\n",
       " 0.04,\n",
       " 0.030714285714285715]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nexamples = [10, 20, 40, 80, 160, 320, 640, 1280, 2560, 5000]\n",
    "error = []\n",
    "p = Perceptron()\n",
    "for n in nexamples:\n",
    "    traininput = Xtrain[:n, :]\n",
    "    trainoutput = ytrain[:n]\n",
    "    # Compute out sample error\n",
    "    error.append(outSample(p, traininput, trainoutput, Xtest, ytest, bias = 0))\n",
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,0,'Number of Training Examples')"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb0385742e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (10, 10))\n",
    "plt.plot(nexamples, error)\n",
    "plt.title('Learning Curve')\n",
    "plt.ylabel('Out of Sample Error')\n",
    "plt.xlabel('Number of Training Examples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above experiment, I chose to plot the out-of-sample error for different numbers of input examples (10, 20, 40, 80, 160, 320, 640, 1280, 2560, 5000). Even before looking at the graph, it seems intuitive that, as we increase the number of examples upon which the classifier is trained, the better it gets. The plot above appears to validate this point. There appears to be a logarithmic decline in the error, with increased number of training examples. It can also be seen from the graph that, it still appears to tend to decrease further. This indicates that, there is more potential to refine the classifier further with even more training examples, and this would lead us to a better classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3:  Data normalization \n",
    "\n",
    "In this section we will explore the effect of normalizing the data, focusing on normalization of features.  The simplest form of normalization is to scale each feature to be in the range [-1, 1].  We'll call this **scaling**.\n",
    "\n",
    "Here's what you need to do:\n",
    "\n",
    "  - Explain how to scale the data to be in the range [-1, 1].\n",
    "  - Compare the accuracy of the perceptron with bias on the original data and the scaled version of the heart dataset.  Does one of them lead to better performance?  Explain why you think this happens.  \n",
    "  - An alternative way of normalizing the data is to **standardize** it:  for each feature subtract the mean and divide by its standard deviation.  What can you say about the range of the resulting features in this case?  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANSWER: #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.a. ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to re-scale data into the range $[-1, 1]$, we can use the following formula on each data example:\n",
    "\n",
    "$$x_{scaled} = \\frac{2 (x - min(x))}{max(x) - min(x)} - 1$$\n",
    "\n",
    "where, \n",
    "\n",
    "* $x$ - original data\n",
    "* $x_{scaled}$ - value of the scaled data\n",
    "* $min(x)$ - data example with the least value in the dataset\n",
    "* $max(x)$ - data example with the largest value in the dataset\n",
    "\n",
    "Let us write a function in Python for this. The function below takes in a 1-D dataset, and scales the values between -1 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.  , -0.75, -0.5 , -0.25,  0.  ,  0.25,  0.5 ,  0.75,  1.  ])"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([1,2,3,4,5,6,7,8,9])\n",
    "def rescale(arr):\n",
    "    new = np.array([])\n",
    "    min = np.amin(a)\n",
    "    max = np.amax(a)\n",
    "    for i in arr:\n",
    "        x = ((2*(i - min))/(max - min)) - 1\n",
    "        new = np.append(new, x)\n",
    "    return new\n",
    "\n",
    "rescale(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.b. ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first look at the error in the case of the unscaled dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unscaled##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in unscaled data.\n",
    "data = np.genfromtxt(\"processed.cleveland.data\", delimiter = \",\")\n",
    "#REMOVE NANS\n",
    "data = data[~np.isnan(data).any(axis=1)]\n",
    "#FIRST STEP IS TO EXTRACT THE LAST COLUMN (TARGET)\n",
    "outdata = data[:,-1]\n",
    "# PROCESS INTO -1 AND 1\n",
    "# TARGET OUTPUTS\n",
    "outdata[outdata > 0] = -1\n",
    "outdata[outdata == 0] = 1\n",
    "# INPUTS\n",
    "indata = np.delete(data, -1, 1)\n",
    "X = indata\n",
    "y = outdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and testing datasets (80 - 20).\n",
    "nRows = X.shape[0]\n",
    "nTrain = int(round(0.8*nRows)) \n",
    "nTest = nRows - nTrain\n",
    "# Shuffle row numbers\n",
    "rows = np.arange(nRows)\n",
    "np.random.shuffle(rows)\n",
    "trainIndices = rows[:nTrain]\n",
    "testIndices = rows[nTrain:]\n",
    "\n",
    "Xtrain = X[trainIndices, :]\n",
    "ytrain = y[trainIndices]\n",
    "Xtest = X[testIndices, :]\n",
    "ytest = y[testIndices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4406779661016949"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate out of sample error\n",
    "p = Perceptron()\n",
    "unscaledError = outSample(p, Xtrain, ytrain, Xtest, ytest)\n",
    "unscaledError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we shall use the rescale function defined above, to scale the heart dataset such that all input values are between -1 and 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.60253906 -1.59863281 -1.59863281 -1.62792969 -1.62402344 -1.609375\n",
      " -1.60351562 -1.60839844 -1.60253906 -1.61230469 -1.60839844 -1.609375\n",
      " -1.609375   -1.62109375 -1.61328125 -1.60839844 -1.6171875  -1.61132812\n",
      " -1.6171875  -1.61621094 -1.6015625  -1.60742188 -1.60742188 -1.60742188\n",
      " -1.60546875 -1.61523438 -1.60742188 -1.59960938 -1.62207031 -1.625\n",
      " -1.59667969 -1.60546875 -1.6015625  -1.60644531 -1.62109375 -1.62304688\n",
      " -1.62207031 -1.60839844 -1.61035156 -1.60449219 -1.60058594 -1.625\n",
      " -1.59472656 -1.60644531 -1.60449219 -1.60742188 -1.61425781 -1.61523438\n",
      " -1.60058594 -1.61230469 -1.62402344 -1.60058594 -1.62109375 -1.62109375\n",
      " -1.60546875 -1.61132812 -1.61523438 -1.62402344 -1.61132812 -1.61425781\n",
      " -1.61425781 -1.61914062 -1.60742188 -1.61132812 -1.61132812 -1.60546875\n",
      " -1.60546875 -1.61132812 -1.60644531 -1.61914062 -1.60058594 -1.59863281\n",
      " -1.60351562 -1.60058594 -1.62109375 -1.60058594 -1.60546875 -1.61425781\n",
      " -1.6171875  -1.60742188 -1.62011719 -1.61230469 -1.62597656 -1.59765625\n",
      " -1.61328125 -1.62109375 -1.61816406 -1.61230469 -1.61425781 -1.59960938\n",
      " -1.60351562 -1.60351562 -1.62109375 -1.60253906 -1.61328125 -1.60644531\n",
      " -1.60546875 -1.61328125 -1.6171875  -1.62011719 -1.63085938 -1.60839844\n",
      " -1.59472656 -1.61621094 -1.61132812 -1.60644531 -1.60839844 -1.60449219\n",
      " -1.62597656 -1.60449219 -1.609375   -1.61328125 -1.62207031 -1.60351562\n",
      " -1.62402344 -1.60742188 -1.62988281 -1.60253906 -1.60058594 -1.6171875\n",
      " -1.60253906 -1.61425781 -1.61035156 -1.60058594 -1.62011719 -1.609375\n",
      " -1.61132812 -1.62109375 -1.60351562 -1.61132812 -1.61425781 -1.63574219\n",
      " -1.61425781 -1.62207031 -1.61035156 -1.59570312 -1.60351562 -1.62988281\n",
      " -1.61425781 -1.60644531 -1.60644531 -1.61328125 -1.6015625  -1.60742188\n",
      " -1.61816406 -1.60839844 -1.62402344 -1.62011719 -1.60546875 -1.61328125\n",
      " -1.62304688 -1.59863281 -1.61035156 -1.6015625  -1.59570312 -1.61425781\n",
      " -1.60742188 -1.60546875 -1.59765625 -1.61914062 -1.58886719 -1.61132812\n",
      " -1.60742188 -1.6171875  -1.60839844 -1.61132812 -1.62988281 -1.62011719\n",
      " -1.59570312 -1.61230469 -1.60644531 -1.60351562 -1.6015625  -1.60839844\n",
      " -1.61328125 -1.609375   -1.62207031 -1.61230469 -1.6171875  -1.609375\n",
      " -1.62304688 -1.60644531 -1.60546875 -1.60253906 -1.62304688 -1.59960938\n",
      " -1.61132812 -1.59667969 -1.61523438 -1.61425781 -1.60351562 -1.59765625\n",
      " -1.59863281 -1.59667969 -1.62011719 -1.61523438 -1.60644531 -1.61523438\n",
      " -1.6015625  -1.60839844 -1.6015625  -1.62207031 -1.62011719 -1.60742188\n",
      " -1.61523438 -1.61035156 -1.60351562 -1.62792969 -1.62695312 -1.62402344\n",
      " -1.59960938 -1.61328125 -1.609375   -1.61914062 -1.61914062 -1.6015625\n",
      " -1.60644531 -1.62402344 -1.61132812 -1.62597656 -1.61230469 -1.60253906\n",
      " -1.63085938 -1.61816406 -1.59863281 -1.61132812 -1.59960938 -1.61328125\n",
      " -1.61035156 -1.61621094 -1.59179688 -1.61132812 -1.61132812 -1.609375\n",
      " -1.61914062 -1.61621094 -1.62304688 -1.62402344 -1.62402344 -1.61621094\n",
      " -1.60449219 -1.60546875 -1.59863281 -1.60742188 -1.61816406 -1.61328125\n",
      " -1.60351562 -1.60839844 -1.60742188 -1.6015625  -1.61425781 -1.62207031\n",
      " -1.62304688 -1.59863281 -1.58984375 -1.59570312 -1.60839844 -1.62109375\n",
      " -1.60742188 -1.60546875 -1.62109375 -1.60449219 -1.62304688 -1.60644531\n",
      " -1.625      -1.62304688 -1.60449219 -1.59960938 -1.61914062 -1.59472656\n",
      " -1.60644531 -1.6015625  -1.59960938 -1.62597656 -1.60839844 -1.60742188\n",
      " -1.60839844 -1.61816406 -1.61035156 -1.62988281 -1.60449219 -1.60742188\n",
      " -1.60742188 -1.609375   -1.609375   -1.59863281 -1.61035156 -1.62109375\n",
      " -1.60253906 -1.60253906 -1.62402344 -1.60644531 -1.60839844 -1.62011719\n",
      " -1.59765625 -1.60839844 -1.60839844]\n",
      "[-1.66308594 -1.66308594 -1.66308594 -1.66308594 -1.6640625  -1.66308594\n",
      " -1.6640625  -1.6640625  -1.66308594 -1.66308594 -1.66308594 -1.6640625\n",
      " -1.66308594 -1.66308594 -1.66308594 -1.66308594 -1.66308594 -1.66308594\n",
      " -1.6640625  -1.66308594 -1.66308594 -1.6640625  -1.66308594 -1.66308594\n",
      " -1.66308594 -1.6640625  -1.6640625  -1.6640625  -1.66308594 -1.66308594\n",
      " -1.6640625  -1.66308594 -1.66308594 -1.66308594 -1.66308594 -1.66308594\n",
      " -1.66308594 -1.66308594 -1.66308594 -1.66308594 -1.6640625  -1.66308594\n",
      " -1.6640625  -1.66308594 -1.6640625  -1.66308594 -1.66308594 -1.66308594\n",
      " -1.6640625  -1.66308594 -1.6640625  -1.66308594 -1.66308594 -1.66308594\n",
      " -1.66308594 -1.66308594 -1.66308594 -1.66308594 -1.66308594 -1.66308594\n",
      " -1.6640625  -1.6640625  -1.66308594 -1.6640625  -1.66308594 -1.66308594\n",
      " -1.66308594 -1.66308594 -1.66308594 -1.66308594 -1.6640625  -1.66308594\n",
      " -1.66308594 -1.66308594 -1.66308594 -1.6640625  -1.66308594 -1.6640625\n",
      " -1.66308594 -1.66308594 -1.66308594 -1.6640625  -1.66308594 -1.66308594\n",
      " -1.66308594 -1.66308594 -1.66308594 -1.6640625  -1.6640625  -1.66308594\n",
      " -1.6640625  -1.66308594 -1.6640625  -1.6640625  -1.66308594 -1.66308594\n",
      " -1.6640625  -1.66308594 -1.66308594 -1.66308594 -1.66308594 -1.6640625\n",
      " -1.6640625  -1.66308594 -1.66308594 -1.66308594 -1.66308594 -1.66308594\n",
      " -1.66308594 -1.6640625  -1.66308594 -1.66308594 -1.6640625  -1.6640625\n",
      " -1.66308594 -1.66308594 -1.6640625  -1.66308594 -1.66308594 -1.66308594\n",
      " -1.6640625  -1.66308594 -1.66308594 -1.66308594 -1.6640625  -1.6640625\n",
      " -1.66308594 -1.66308594 -1.6640625  -1.66308594 -1.66308594 -1.66308594\n",
      " -1.66308594 -1.6640625  -1.6640625  -1.66308594 -1.66308594 -1.66308594\n",
      " -1.66308594 -1.66308594 -1.66308594 -1.66308594 -1.66308594 -1.66308594\n",
      " -1.66308594 -1.66308594 -1.66308594 -1.66308594 -1.6640625  -1.66308594\n",
      " -1.6640625  -1.6640625  -1.66308594 -1.66308594 -1.66308594 -1.66308594\n",
      " -1.66308594 -1.66308594 -1.66308594 -1.66308594 -1.66308594 -1.6640625\n",
      " -1.6640625  -1.66308594 -1.66308594 -1.6640625  -1.66308594 -1.6640625\n",
      " -1.66308594 -1.66308594 -1.6640625  -1.6640625  -1.66308594 -1.66308594\n",
      " -1.66308594 -1.66308594 -1.66308594 -1.66308594 -1.66308594 -1.6640625\n",
      " -1.66308594 -1.66308594 -1.6640625  -1.6640625  -1.66308594 -1.66308594\n",
      " -1.66308594 -1.66308594 -1.66308594 -1.66308594 -1.6640625  -1.6640625\n",
      " -1.66308594 -1.66308594 -1.6640625  -1.6640625  -1.66308594 -1.6640625\n",
      " -1.6640625  -1.66308594 -1.6640625  -1.66308594 -1.66308594 -1.66308594\n",
      " -1.66308594 -1.66308594 -1.6640625  -1.6640625  -1.66308594 -1.66308594\n",
      " -1.6640625  -1.66308594 -1.66308594 -1.6640625  -1.6640625  -1.6640625\n",
      " -1.66308594 -1.6640625  -1.6640625  -1.6640625  -1.66308594 -1.6640625\n",
      " -1.6640625  -1.66308594 -1.6640625  -1.66308594 -1.66308594 -1.6640625\n",
      " -1.6640625  -1.66308594 -1.6640625  -1.6640625  -1.66308594 -1.66308594\n",
      " -1.66308594 -1.6640625  -1.66308594 -1.66308594 -1.6640625  -1.6640625\n",
      " -1.66308594 -1.6640625  -1.66308594 -1.66308594 -1.66308594 -1.66308594\n",
      " -1.66308594 -1.66308594 -1.66308594 -1.66308594 -1.6640625  -1.66308594\n",
      " -1.6640625  -1.6640625  -1.6640625  -1.66308594 -1.66308594 -1.6640625\n",
      " -1.6640625  -1.6640625  -1.66308594 -1.66308594 -1.66308594 -1.66308594\n",
      " -1.66308594 -1.66308594 -1.66308594 -1.66308594 -1.66308594 -1.6640625\n",
      " -1.66308594 -1.66308594 -1.6640625  -1.6640625  -1.66308594 -1.6640625\n",
      " -1.66308594 -1.66308594 -1.6640625  -1.66308594 -1.66308594 -1.66308594\n",
      " -1.6640625  -1.66308594 -1.66308594 -1.66308594 -1.6640625  -1.66308594\n",
      " -1.66308594 -1.6640625  -1.66308594 -1.66308594 -1.6640625  -1.66308594\n",
      " -1.66308594 -1.66308594 -1.6640625 ]\n",
      "[-1.66308594 -1.66015625 -1.66015625 -1.66113281 -1.66210938 -1.66210938\n",
      " -1.66015625 -1.66015625 -1.66015625 -1.66015625 -1.66015625 -1.66210938\n",
      " -1.66113281 -1.66210938 -1.66113281 -1.66113281 -1.66210938 -1.66015625\n",
      " -1.66113281 -1.66210938 -1.66308594 -1.66308594 -1.66210938 -1.66113281\n",
      " -1.66015625 -1.66113281 -1.66113281 -1.66308594 -1.66015625 -1.66015625\n",
      " -1.66308594 -1.66015625 -1.66113281 -1.66015625 -1.66113281 -1.66015625\n",
      " -1.66015625 -1.66015625 -1.66015625 -1.66113281 -1.66015625 -1.66308594\n",
      " -1.66210938 -1.66113281 -1.66015625 -1.66113281 -1.66113281 -1.66015625\n",
      " -1.66113281 -1.66113281 -1.66210938 -1.66015625 -1.66015625 -1.66210938\n",
      " -1.66015625 -1.66015625 -1.66113281 -1.66015625 -1.66113281 -1.66308594\n",
      " -1.66015625 -1.66113281 -1.66015625 -1.66113281 -1.66015625 -1.66015625\n",
      " -1.66113281 -1.66113281 -1.66015625 -1.66113281 -1.66113281 -1.66015625\n",
      " -1.66015625 -1.66015625 -1.66015625 -1.66113281 -1.66015625 -1.66113281\n",
      " -1.66210938 -1.66015625 -1.66015625 -1.66015625 -1.66113281 -1.66113281\n",
      " -1.66210938 -1.66113281 -1.66113281 -1.66015625 -1.66113281 -1.66015625\n",
      " -1.66015625 -1.66113281 -1.66113281 -1.66113281 -1.66015625 -1.66015625\n",
      " -1.66015625 -1.66210938 -1.66015625 -1.66015625 -1.66308594 -1.66015625\n",
      " -1.66113281 -1.66113281 -1.66210938 -1.66015625 -1.66113281 -1.66015625\n",
      " -1.66015625 -1.66015625 -1.66015625 -1.66308594 -1.66015625 -1.66113281\n",
      " -1.66210938 -1.66113281 -1.66015625 -1.66015625 -1.66015625 -1.66015625\n",
      " -1.66015625 -1.66113281 -1.66015625 -1.66308594 -1.66210938 -1.66015625\n",
      " -1.66015625 -1.66210938 -1.66015625 -1.66113281 -1.66113281 -1.66210938\n",
      " -1.66015625 -1.66113281 -1.66210938 -1.66015625 -1.66210938 -1.66015625\n",
      " -1.66113281 -1.66210938 -1.66308594 -1.66210938 -1.66113281 -1.66113281\n",
      " -1.66113281 -1.66015625 -1.66113281 -1.66210938 -1.66113281 -1.66308594\n",
      " -1.66015625 -1.66113281 -1.66015625 -1.66015625 -1.66015625 -1.66015625\n",
      " -1.66015625 -1.66015625 -1.66113281 -1.66210938 -1.66015625 -1.66113281\n",
      " -1.66015625 -1.66113281 -1.66015625 -1.66210938 -1.66015625 -1.66210938\n",
      " -1.66113281 -1.66015625 -1.66015625 -1.66015625 -1.66015625 -1.66015625\n",
      " -1.66015625 -1.66015625 -1.66113281 -1.66113281 -1.66015625 -1.66015625\n",
      " -1.66308594 -1.66308594 -1.66015625 -1.66210938 -1.66113281 -1.66210938\n",
      " -1.66210938 -1.66113281 -1.66113281 -1.66015625 -1.66015625 -1.66113281\n",
      " -1.66015625 -1.66308594 -1.66015625 -1.66210938 -1.66308594 -1.66015625\n",
      " -1.66015625 -1.66113281 -1.66113281 -1.66015625 -1.66015625 -1.66015625\n",
      " -1.66015625 -1.66210938 -1.66015625 -1.66113281 -1.66308594 -1.66113281\n",
      " -1.66015625 -1.66015625 -1.66308594 -1.66210938 -1.66015625 -1.66015625\n",
      " -1.66015625 -1.66113281 -1.66113281 -1.66113281 -1.66015625 -1.66015625\n",
      " -1.66210938 -1.66015625 -1.66113281 -1.66015625 -1.66015625 -1.66113281\n",
      " -1.66015625 -1.66113281 -1.66210938 -1.66113281 -1.66015625 -1.66015625\n",
      " -1.66015625 -1.66210938 -1.66210938 -1.66210938 -1.66210938 -1.66015625\n",
      " -1.66308594 -1.66113281 -1.66015625 -1.66015625 -1.66015625 -1.66015625\n",
      " -1.66210938 -1.66015625 -1.66015625 -1.66015625 -1.66113281 -1.66015625\n",
      " -1.66113281 -1.66015625 -1.66113281 -1.66210938 -1.66210938 -1.66113281\n",
      " -1.66210938 -1.66308594 -1.66113281 -1.66015625 -1.66015625 -1.66113281\n",
      " -1.66015625 -1.66113281 -1.66015625 -1.66015625 -1.66015625 -1.66015625\n",
      " -1.66308594 -1.66308594 -1.66113281 -1.66113281 -1.66210938 -1.66015625\n",
      " -1.66015625 -1.66113281 -1.66015625 -1.66210938 -1.66015625 -1.66015625\n",
      " -1.66015625 -1.66210938 -1.66210938 -1.66113281 -1.66210938 -1.66015625\n",
      " -1.66015625 -1.66015625 -1.66210938 -1.66015625 -1.66015625 -1.66308594\n",
      " -1.66015625 -1.66015625 -1.66210938]\n",
      "[-1.52246094 -1.5078125  -1.546875   -1.53710938 -1.53710938 -1.546875\n",
      " -1.52734375 -1.546875   -1.53710938 -1.52734375 -1.52734375 -1.52734375\n",
      " -1.53710938 -1.546875   -1.49609375 -1.51757812 -1.55664062 -1.52734375\n",
      " -1.53710938 -1.53710938 -1.55664062 -1.51757812 -1.546875   -1.53515625\n",
      " -1.53710938 -1.546875   -1.546875   -1.51757812 -1.51757812 -1.55664062\n",
      " -1.52734375 -1.54980469 -1.52734375 -1.53222656 -1.53710938 -1.52734375\n",
      " -1.546875   -1.51757812 -1.53515625 -1.51757812 -1.51757812 -1.52734375\n",
      " -1.5078125  -1.51757812 -1.53710938 -1.5546875  -1.55664062 -1.51757812\n",
      " -1.52734375 -1.53710938 -1.56152344 -1.546875   -1.5546875  -1.53710938\n",
      " -1.53710938 -1.54296875 -1.52734375 -1.55664062 -1.54199219 -1.54199219\n",
      " -1.53710938 -1.52539062 -1.5390625  -1.53222656 -1.546875   -1.52246094\n",
      " -1.52734375 -1.51757812 -1.49804688 -1.51757812 -1.51269531 -1.54199219\n",
      " -1.546875   -1.55664062 -1.55664062 -1.5078125  -1.54199219 -1.52734375\n",
      " -1.53710938 -1.51757812 -1.5625     -1.53710938 -1.52734375 -1.48828125\n",
      " -1.546875   -1.52734375 -1.52929688 -1.52929688 -1.53710938 -1.546875\n",
      " -1.5078125  -1.53710938 -1.55859375 -1.53222656 -1.5390625  -1.55664062\n",
      " -1.51757812 -1.53320312 -1.54492188 -1.55175781 -1.54882812 -1.5390625\n",
      " -1.55664062 -1.546875   -1.55859375 -1.52734375 -1.5390625  -1.546875\n",
      " -1.54882812 -1.52246094 -1.54199219 -1.54882812 -1.53515625 -1.53710938\n",
      " -1.53222656 -1.52734375 -1.52929688 -1.53710938 -1.53222656 -1.53710938\n",
      " -1.51757812 -1.56640625 -1.52734375 -1.52929688 -1.53710938 -1.46875\n",
      " -1.55664062 -1.546875   -1.54296875 -1.546875   -1.57226562 -1.53710938\n",
      " -1.52734375 -1.54492188 -1.53222656 -1.52246094 -1.546875   -1.546875\n",
      " -1.54199219 -1.52734375 -1.49804688 -1.5390625  -1.54199219 -1.56152344\n",
      " -1.55859375 -1.50292969 -1.5546875  -1.5390625  -1.56445312 -1.515625\n",
      " -1.56445312 -1.55175781 -1.5078125  -1.546875   -1.53710938 -1.52734375\n",
      " -1.54199219 -1.52734375 -1.54882812 -1.56542969 -1.54199219 -1.55664062\n",
      " -1.56640625 -1.54296875 -1.53515625 -1.53515625 -1.54101562 -1.5546875\n",
      " -1.5078125  -1.52539062 -1.49414062 -1.52734375 -1.52246094 -1.515625\n",
      " -1.55859375 -1.53515625 -1.53710938 -1.53710938 -1.54296875 -1.53320312\n",
      " -1.51953125 -1.49023438 -1.50976562 -1.52734375 -1.546875   -1.5078125\n",
      " -1.4765625  -1.52734375 -1.53808594 -1.52734375 -1.52929688 -1.546875\n",
      " -1.56640625 -1.5078125  -1.52929688 -1.546875   -1.5078125  -1.55664062\n",
      " -1.48828125 -1.51757812 -1.52734375 -1.55664062 -1.52539062 -1.5390625\n",
      " -1.5234375  -1.53710938 -1.51757812 -1.546875   -1.546875   -1.53710938\n",
      " -1.49023438 -1.5546875  -1.546875   -1.56152344 -1.52929688 -1.53710938\n",
      " -1.52929688 -1.5546875  -1.55859375 -1.57226562 -1.54394531 -1.55859375\n",
      " -1.54882812 -1.5546875  -1.515625   -1.55664062 -1.5546875  -1.53125\n",
      " -1.48828125 -1.54882812 -1.546875   -1.5078125  -1.54492188 -1.53710938\n",
      " -1.546875   -1.53320312 -1.546875   -1.55664062 -1.54101562 -1.53710938\n",
      " -1.53320312 -1.546875   -1.546875   -1.56640625 -1.55664062 -1.54199219\n",
      " -1.5390625  -1.55664062 -1.52148438 -1.5390625  -1.546875   -1.55175781\n",
      " -1.546875   -1.56054688 -1.52734375 -1.51171875 -1.54296875 -1.54882812\n",
      " -1.53125    -1.51757812 -1.546875   -1.52929688 -1.53125    -1.54101562\n",
      " -1.515625   -1.53710938 -1.52734375 -1.5078125  -1.52734375 -1.5546875\n",
      " -1.53320312 -1.49804688 -1.52148438 -1.52929688 -1.51367188 -1.53710938\n",
      " -1.55664062 -1.53710938 -1.5390625  -1.54492188 -1.51953125 -1.55273438\n",
      " -1.49804688 -1.53710938 -1.546875   -1.515625   -1.53515625 -1.546875\n",
      " -1.52734375 -1.54296875 -1.546875   -1.50390625 -1.52734375 -1.55664062\n",
      " -1.5234375  -1.53710938 -1.53710938]\n",
      "[-1.43652344 -1.38476562 -1.44042969 -1.41992188 -1.46484375 -1.43359375\n",
      " -1.40234375 -1.31835938 -1.41601562 -1.46582031 -1.4765625  -1.37695312\n",
      " -1.4140625  -1.40722656 -1.46972656 -1.5        -1.44042969 -1.43066406\n",
      " -1.39550781 -1.40429688 -1.45800781 -1.38769531 -1.38671875 -1.4453125\n",
      " -1.46289062 -1.45019531 -1.33203125 -1.44335938 -1.42285156 -1.50097656\n",
      " -1.43066406 -1.43945312 -1.33691406 -1.43554688 -1.43652344 -1.44335938\n",
      " -1.49121094 -1.39453125 -1.31933594 -1.42675781 -1.44433594 -1.46972656\n",
      " -1.36914062 -1.45703125 -1.34179688 -1.43945312 -1.49316406 -1.42675781\n",
      " -1.25683594 -1.47167969 -1.47070312 -1.49121094 -1.38085938 -1.45019531\n",
      " -1.41699219 -1.40429688 -1.43652344 -1.49609375 -1.39746094 -1.45605469\n",
      " -1.36621094 -1.49121094 -1.453125   -1.3671875  -1.48046875 -1.38867188\n",
      " -1.48339844 -1.4375     -1.34570312 -1.43847656 -1.40136719 -1.41601562\n",
      " -1.40332031 -1.421875   -1.47167969 -1.3125     -1.41210938 -1.36328125\n",
      " -1.42480469 -1.40039062 -1.4609375  -1.40625    -1.35058594 -1.39648438\n",
      " -1.34667969 -1.43457031 -1.41308594 -1.43554688 -1.4140625  -1.36914062\n",
      " -1.50390625 -1.43847656 -1.52636719 -1.41796875 -1.41503906 -1.43066406\n",
      " -1.41210938 -1.46777344 -1.44726562 -1.41015625 -1.48632812 -1.36816406\n",
      " -1.40527344 -1.48046875 -1.36230469 -1.49121094 -1.44042969 -1.41015625\n",
      " -1.45019531 -1.36425781 -1.42089844 -1.48242188 -1.33105469 -1.40722656\n",
      " -1.46582031 -1.45800781 -1.48535156 -1.34179688 -1.41601562 -1.4140625\n",
      " -1.26660156 -1.44726562 -1.45214844 -1.38867188 -1.43554688 -1.3828125\n",
      " -1.43066406 -1.44921875 -1.45996094 -1.41210938 -1.44238281 -1.46484375\n",
      " -1.40917969 -1.45605469 -1.41992188 -1.49414062 -1.38964844 -1.47070312\n",
      " -1.42480469 -1.44824219 -1.3828125  -1.46386719 -1.36230469 -1.4296875\n",
      " -1.42675781 -1.38183594 -1.41992188 -1.36328125 -1.35351562 -1.37304688\n",
      " -1.40527344 -1.11328125 -1.38183594 -1.42382812 -1.34960938 -1.37207031\n",
      " -1.37109375 -1.37792969 -1.39355469 -1.47167969 -1.3671875  -1.45507812\n",
      " -1.421875   -1.41503906 -1.46191406 -1.3828125  -1.38867188 -1.5078125\n",
      " -1.40136719 -1.44335938 -1.42089844 -1.27929688 -1.45703125 -1.39648438\n",
      " -1.43652344 -1.484375   -1.35644531 -1.42382812 -1.39648438 -1.26464844\n",
      " -1.42578125 -1.40039062 -1.36621094 -1.47363281 -1.4296875  -1.42382812\n",
      " -1.38769531 -1.41601562 -1.47265625 -1.37304688 -1.37695312 -1.45800781\n",
      " -1.37207031 -1.43554688 -1.43359375 -1.42578125 -1.39746094 -1.41601562\n",
      " -1.34667969 -1.54101562 -1.35839844 -1.45800781 -1.36230469 -1.41113281\n",
      " -1.46875    -1.40820312 -1.42578125 -1.45410156 -1.43847656 -1.45507812\n",
      " -1.44140625 -1.43945312 -1.47558594 -1.46484375 -1.42675781 -1.36816406\n",
      " -1.39941406 -1.40234375 -1.40332031 -1.46972656 -1.38867188 -1.40136719\n",
      " -1.45898438 -1.46484375 -1.39355469 -1.46289062 -1.45703125 -1.47265625\n",
      " -1.34472656 -1.51855469 -1.40136719 -1.46777344 -1.38476562 -1.38769531\n",
      " -1.42089844 -1.39941406 -1.37597656 -1.43457031 -1.36523438 -1.40136719\n",
      " -1.43554688 -1.49023438 -1.43261719 -1.43554688 -1.39550781 -1.45703125\n",
      " -1.4609375  -1.46777344 -1.45117188 -1.40722656 -1.37597656 -1.36816406\n",
      " -1.45996094 -1.44628906 -1.47167969 -1.42480469 -1.40917969 -1.42773438\n",
      " -1.35253906 -1.4296875  -1.44335938 -1.50195312 -1.35644531 -1.45117188\n",
      " -1.44628906 -1.48828125 -1.46191406 -1.44140625 -1.36035156 -1.51855469\n",
      " -1.46484375 -1.44238281 -1.39257812 -1.44921875 -1.4375     -1.47167969\n",
      " -1.33691406 -1.41699219 -1.46386719 -1.4765625  -1.46582031 -1.35351562\n",
      " -1.44433594 -1.44824219 -1.4296875  -1.45703125 -1.33007812 -1.49902344\n",
      " -1.48144531 -1.47167969 -1.51074219 -1.4921875  -1.42871094 -1.40625\n",
      " -1.47558594 -1.53613281 -1.43359375]\n",
      "[-1.66308594 -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.6640625\n",
      " -1.6640625  -1.6640625  -1.6640625  -1.66308594 -1.6640625  -1.6640625\n",
      " -1.66308594 -1.6640625  -1.66308594 -1.6640625  -1.6640625  -1.6640625\n",
      " -1.6640625  -1.6640625  -1.6640625  -1.66308594 -1.6640625  -1.6640625\n",
      " -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.6640625\n",
      " -1.6640625  -1.66308594 -1.6640625  -1.6640625  -1.6640625  -1.6640625\n",
      " -1.6640625  -1.6640625  -1.6640625  -1.66308594 -1.6640625  -1.6640625\n",
      " -1.6640625  -1.66308594 -1.6640625  -1.6640625  -1.6640625  -1.6640625\n",
      " -1.66308594 -1.66308594 -1.6640625  -1.6640625  -1.6640625  -1.6640625\n",
      " -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.6640625\n",
      " -1.6640625  -1.6640625  -1.6640625  -1.66308594 -1.6640625  -1.6640625\n",
      " -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.66308594\n",
      " -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.6640625\n",
      " -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.66308594\n",
      " -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.6640625\n",
      " -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.6640625\n",
      " -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.6640625\n",
      " -1.66308594 -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.6640625\n",
      " -1.6640625  -1.6640625  -1.66308594 -1.6640625  -1.66308594 -1.6640625\n",
      " -1.6640625  -1.66308594 -1.6640625  -1.66308594 -1.6640625  -1.66308594\n",
      " -1.6640625  -1.6640625  -1.6640625  -1.66308594 -1.6640625  -1.66308594\n",
      " -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.6640625\n",
      " -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.6640625\n",
      " -1.66308594 -1.6640625  -1.6640625  -1.66308594 -1.6640625  -1.6640625\n",
      " -1.6640625  -1.66308594 -1.6640625  -1.6640625  -1.6640625  -1.66308594\n",
      " -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.6640625\n",
      " -1.6640625  -1.6640625  -1.6640625  -1.66308594 -1.6640625  -1.6640625\n",
      " -1.6640625  -1.66308594 -1.6640625  -1.66308594 -1.6640625  -1.6640625\n",
      " -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.6640625\n",
      " -1.66308594 -1.6640625  -1.6640625  -1.66308594 -1.6640625  -1.6640625\n",
      " -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.66308594 -1.6640625\n",
      " -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.66308594 -1.6640625\n",
      " -1.6640625  -1.66308594 -1.6640625  -1.6640625  -1.6640625  -1.6640625\n",
      " -1.6640625  -1.66308594 -1.6640625  -1.6640625  -1.6640625  -1.6640625\n",
      " -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.6640625\n",
      " -1.66308594 -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.6640625\n",
      " -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.6640625\n",
      " -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.6640625\n",
      " -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.66308594\n",
      " -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.6640625\n",
      " -1.6640625  -1.66308594 -1.6640625  -1.6640625  -1.6640625  -1.6640625\n",
      " -1.66308594 -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.6640625\n",
      " -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.6640625\n",
      " -1.66308594 -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.66308594\n",
      " -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.6640625\n",
      " -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.6640625\n",
      " -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.6640625\n",
      " -1.66308594 -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.6640625\n",
      " -1.6640625  -1.6640625  -1.6640625  -1.66308594 -1.6640625  -1.6640625\n",
      " -1.66308594 -1.6640625  -1.6640625 ]\n",
      "[-1.66210938 -1.66210938 -1.66210938 -1.6640625  -1.66210938 -1.6640625\n",
      " -1.66210938 -1.6640625  -1.66210938 -1.66210938 -1.6640625  -1.66210938\n",
      " -1.66210938 -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.6640625\n",
      " -1.6640625  -1.6640625  -1.66210938 -1.66210938 -1.66210938 -1.66210938\n",
      " -1.66210938 -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.66210938\n",
      " -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.6640625\n",
      " -1.66210938 -1.66210938 -1.6640625  -1.6640625  -1.66210938 -1.6640625\n",
      " -1.6640625  -1.6640625  -1.66210938 -1.66210938 -1.6640625  -1.66210938\n",
      " -1.66210938 -1.66210938 -1.6640625  -1.6640625  -1.66210938 -1.66210938\n",
      " -1.6640625  -1.66210938 -1.6640625  -1.66210938 -1.66210938 -1.66210938\n",
      " -1.6640625  -1.66210938 -1.66210938 -1.6640625  -1.6640625  -1.66210938\n",
      " -1.66210938 -1.66210938 -1.66210938 -1.6640625  -1.6640625  -1.6640625\n",
      " -1.6640625  -1.66210938 -1.66210938 -1.66210938 -1.66210938 -1.66210938\n",
      " -1.66210938 -1.66210938 -1.66210938 -1.66210938 -1.66210938 -1.66210938\n",
      " -1.6640625  -1.66210938 -1.66210938 -1.66210938 -1.66210938 -1.66210938\n",
      " -1.66210938 -1.6640625  -1.6640625  -1.66210938 -1.6640625  -1.66210938\n",
      " -1.66210938 -1.6640625  -1.66210938 -1.66210938 -1.66210938 -1.66210938\n",
      " -1.66210938 -1.6640625  -1.6640625  -1.6640625  -1.66210938 -1.6640625\n",
      " -1.6640625  -1.66210938 -1.66210938 -1.66210938 -1.66210938 -1.6640625\n",
      " -1.6640625  -1.66210938 -1.6640625  -1.66210938 -1.66210938 -1.66210938\n",
      " -1.66210938 -1.6640625  -1.6640625  -1.66210938 -1.66210938 -1.66210938\n",
      " -1.6640625  -1.6640625  -1.6640625  -1.66210938 -1.6640625  -1.66210938\n",
      " -1.66210938 -1.6640625  -1.66210938 -1.6640625  -1.66210938 -1.6640625\n",
      " -1.66210938 -1.6640625  -1.66210938 -1.6640625  -1.6640625  -1.66210938\n",
      " -1.6640625  -1.66210938 -1.6640625  -1.66210938 -1.6640625  -1.6640625\n",
      " -1.66210938 -1.66210938 -1.66210938 -1.66210938 -1.66210938 -1.6640625\n",
      " -1.66210938 -1.66210938 -1.6640625  -1.6640625  -1.66210938 -1.6640625\n",
      " -1.66210938 -1.6640625  -1.6640625  -1.66210938 -1.66210938 -1.6640625\n",
      " -1.6640625  -1.66210938 -1.6640625  -1.66210938 -1.66210938 -1.6640625\n",
      " -1.6640625  -1.66210938 -1.6640625  -1.66210938 -1.66210938 -1.66210938\n",
      " -1.66210938 -1.66210938 -1.66210938 -1.6640625  -1.6640625  -1.6640625\n",
      " -1.66210938 -1.66210938 -1.6640625  -1.6640625  -1.6640625  -1.66210938\n",
      " -1.66210938 -1.66210938 -1.66210938 -1.6640625  -1.66210938 -1.66210938\n",
      " -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.66210938 -1.66210938\n",
      " -1.66210938 -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.66210938\n",
      " -1.6640625  -1.6640625  -1.66210938 -1.6640625  -1.66210938 -1.6640625\n",
      " -1.66210938 -1.66210938 -1.66210938 -1.6640625  -1.6640625  -1.6640625\n",
      " -1.6640625  -1.6640625  -1.6640625  -1.66210938 -1.66210938 -1.66210938\n",
      " -1.66308594 -1.66210938 -1.66210938 -1.6640625  -1.66210938 -1.66210938\n",
      " -1.66210938 -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.6640625\n",
      " -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.66210938 -1.6640625\n",
      " -1.66210938 -1.6640625  -1.6640625  -1.6640625  -1.66210938 -1.6640625\n",
      " -1.6640625  -1.6640625  -1.66308594 -1.66210938 -1.6640625  -1.6640625\n",
      " -1.66210938 -1.6640625  -1.6640625  -1.66210938 -1.6640625  -1.6640625\n",
      " -1.6640625  -1.6640625  -1.66210938 -1.66210938 -1.6640625  -1.6640625\n",
      " -1.6640625  -1.66210938 -1.66210938 -1.6640625  -1.66210938 -1.6640625\n",
      " -1.6640625  -1.6640625  -1.66308594 -1.6640625  -1.6640625  -1.66308594\n",
      " -1.66210938 -1.66210938 -1.6640625  -1.66210938 -1.6640625  -1.6640625\n",
      " -1.66210938 -1.6640625  -1.6640625  -1.66210938 -1.6640625  -1.6640625\n",
      " -1.6640625  -1.6640625  -1.66210938]\n",
      "[-1.51757812 -1.55859375 -1.53808594 -1.48144531 -1.49609375 -1.49023438\n",
      " -1.5078125  -1.50488281 -1.52050781 -1.51269531 -1.51953125 -1.51464844\n",
      " -1.52539062 -1.49511719 -1.50585938 -1.49414062 -1.5        -1.5078125\n",
      " -1.52832031 -1.49707031 -1.5234375  -1.50585938 -1.5078125  -1.49511719\n",
      " -1.53515625 -1.50976562 -1.49609375 -1.55273438 -1.49707031 -1.55273438\n",
      " -1.51660156 -1.5078125  -1.50976562 -1.50683594 -1.48925781 -1.49023438\n",
      " -1.546875   -1.5546875  -1.53515625 -1.53027344 -1.55273438 -1.49023438\n",
      " -1.50585938 -1.51074219 -1.49902344 -1.50292969 -1.54394531 -1.5390625\n",
      " -1.51074219 -1.515625   -1.5        -1.52734375 -1.51464844 -1.48046875\n",
      " -1.5234375  -1.55761719 -1.50488281 -1.50976562 -1.515625   -1.54199219\n",
      " -1.52539062 -1.5078125  -1.53613281 -1.49804688 -1.55371094 -1.52539062\n",
      " -1.51269531 -1.50292969 -1.52734375 -1.52050781 -1.51953125 -1.50488281\n",
      " -1.56738281 -1.50976562 -1.49121094 -1.51660156 -1.52636719 -1.52539062\n",
      " -1.48828125 -1.55566406 -1.51953125 -1.52441406 -1.48632812 -1.51757812\n",
      " -1.49609375 -1.48828125 -1.51171875 -1.5078125  -1.51855469 -1.51660156\n",
      " -1.52246094 -1.52148438 -1.49316406 -1.49609375 -1.50683594 -1.52539062\n",
      " -1.51074219 -1.50976562 -1.48242188 -1.48339844 -1.49414062 -1.50878906\n",
      " -1.53710938 -1.52832031 -1.51171875 -1.50585938 -1.51757812 -1.52734375\n",
      " -1.52734375 -1.52148438 -1.5234375  -1.47851562 -1.53125    -1.56933594\n",
      " -1.53515625 -1.50292969 -1.48632812 -1.53515625 -1.54003906 -1.51757812\n",
      " -1.51367188 -1.52441406 -1.55566406 -1.49414062 -1.49316406 -1.53417969\n",
      " -1.54101562 -1.49804688 -1.50488281 -1.52050781 -1.51367188 -1.46679688\n",
      " -1.48242188 -1.50292969 -1.50683594 -1.54199219 -1.56347656 -1.53710938\n",
      " -1.50195312 -1.50390625 -1.50878906 -1.484375   -1.53613281 -1.51367188\n",
      " -1.515625   -1.54296875 -1.48925781 -1.49804688 -1.5078125  -1.49023438\n",
      " -1.54492188 -1.5078125  -1.52246094 -1.5703125  -1.55761719 -1.49511719\n",
      " -1.49707031 -1.49804688 -1.51660156 -1.51171875 -1.50585938 -1.50976562\n",
      " -1.54492188 -1.49316406 -1.5        -1.50878906 -1.51171875 -1.52929688\n",
      " -1.5546875  -1.55566406 -1.52441406 -1.51074219 -1.53515625 -1.578125\n",
      " -1.52050781 -1.56152344 -1.50585938 -1.49511719 -1.50195312 -1.51757812\n",
      " -1.49023438 -1.52246094 -1.50683594 -1.48925781 -1.47460938 -1.546875\n",
      " -1.47363281 -1.52148438 -1.50488281 -1.54492188 -1.56054688 -1.55175781\n",
      " -1.54199219 -1.53613281 -1.515625   -1.50585938 -1.54199219 -1.50878906\n",
      " -1.51367188 -1.49511719 -1.53417969 -1.50683594 -1.52050781 -1.53710938\n",
      " -1.54101562 -1.51269531 -1.51367188 -1.49804688 -1.48632812 -1.5\n",
      " -1.50292969 -1.5078125  -1.50585938 -1.49609375 -1.515625   -1.54492188\n",
      " -1.48632812 -1.49609375 -1.50097656 -1.48925781 -1.57128906 -1.49902344\n",
      " -1.4765625  -1.52441406 -1.49609375 -1.55859375 -1.53515625 -1.49902344\n",
      " -1.54980469 -1.54101562 -1.54589844 -1.50488281 -1.55078125 -1.56347656\n",
      " -1.5234375  -1.50585938 -1.50585938 -1.51464844 -1.50488281 -1.50488281\n",
      " -1.52246094 -1.5703125  -1.59472656 -1.51171875 -1.54882812 -1.5\n",
      " -1.52734375 -1.54101562 -1.56152344 -1.56152344 -1.51074219 -1.48730469\n",
      " -1.49511719 -1.52539062 -1.55078125 -1.52441406 -1.52636719 -1.51855469\n",
      " -1.515625   -1.49707031 -1.49902344 -1.54199219 -1.54199219 -1.53320312\n",
      " -1.48730469 -1.51757812 -1.52929688 -1.52929688 -1.546875   -1.54199219\n",
      " -1.50585938 -1.51269531 -1.515625   -1.515625   -1.50390625 -1.53613281\n",
      " -1.52441406 -1.48925781 -1.53710938 -1.49414062 -1.50683594 -1.52734375\n",
      " -1.52148438 -1.50488281 -1.49902344 -1.51757812 -1.50195312 -1.5234375\n",
      " -1.5234375  -1.53125    -1.48632812 -1.57617188 -1.54394531 -1.53515625\n",
      " -1.52636719 -1.55175781 -1.49414062]\n",
      "[-1.6640625  -1.66308594 -1.66308594 -1.6640625  -1.6640625  -1.6640625\n",
      " -1.6640625  -1.66308594 -1.6640625  -1.66308594 -1.6640625  -1.6640625\n",
      " -1.66308594 -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.6640625\n",
      " -1.6640625  -1.6640625  -1.66308594 -1.6640625  -1.6640625  -1.6640625\n",
      " -1.66308594 -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.66308594\n",
      " -1.6640625  -1.66308594 -1.6640625  -1.6640625  -1.66308594 -1.6640625\n",
      " -1.66308594 -1.66308594 -1.66308594 -1.66308594 -1.6640625  -1.66308594\n",
      " -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.6640625\n",
      " -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.6640625\n",
      " -1.66308594 -1.66308594 -1.6640625  -1.6640625  -1.6640625  -1.66308594\n",
      " -1.66308594 -1.66308594 -1.66308594 -1.6640625  -1.6640625  -1.66308594\n",
      " -1.6640625  -1.6640625  -1.66308594 -1.6640625  -1.6640625  -1.6640625\n",
      " -1.66308594 -1.6640625  -1.6640625  -1.6640625  -1.66308594 -1.6640625\n",
      " -1.6640625  -1.66308594 -1.66308594 -1.6640625  -1.6640625  -1.66308594\n",
      " -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.6640625\n",
      " -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.66308594 -1.66308594\n",
      " -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.6640625\n",
      " -1.6640625  -1.6640625  -1.6640625  -1.66308594 -1.6640625  -1.66308594\n",
      " -1.6640625  -1.66308594 -1.66308594 -1.6640625  -1.66308594 -1.6640625\n",
      " -1.6640625  -1.6640625  -1.6640625  -1.66308594 -1.6640625  -1.66308594\n",
      " -1.6640625  -1.66308594 -1.66308594 -1.6640625  -1.6640625  -1.66308594\n",
      " -1.66308594 -1.6640625  -1.6640625  -1.6640625  -1.66308594 -1.6640625\n",
      " -1.66308594 -1.6640625  -1.6640625  -1.66308594 -1.6640625  -1.66308594\n",
      " -1.6640625  -1.66308594 -1.6640625  -1.6640625  -1.66308594 -1.66308594\n",
      " -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.6640625\n",
      " -1.6640625  -1.6640625  -1.66308594 -1.66308594 -1.6640625  -1.66308594\n",
      " -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.66308594 -1.6640625\n",
      " -1.6640625  -1.6640625  -1.66308594 -1.66308594 -1.66308594 -1.6640625\n",
      " -1.66308594 -1.66308594 -1.66308594 -1.6640625  -1.6640625  -1.66308594\n",
      " -1.6640625  -1.66308594 -1.6640625  -1.6640625  -1.6640625  -1.66308594\n",
      " -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.66308594\n",
      " -1.6640625  -1.6640625  -1.6640625  -1.66308594 -1.6640625  -1.6640625\n",
      " -1.66308594 -1.6640625  -1.66308594 -1.6640625  -1.6640625  -1.6640625\n",
      " -1.66308594 -1.6640625  -1.6640625  -1.6640625  -1.66308594 -1.66308594\n",
      " -1.66308594 -1.6640625  -1.66308594 -1.6640625  -1.66308594 -1.6640625\n",
      " -1.66308594 -1.6640625  -1.6640625  -1.6640625  -1.66308594 -1.6640625\n",
      " -1.6640625  -1.66308594 -1.6640625  -1.6640625  -1.66308594 -1.66308594\n",
      " -1.6640625  -1.6640625  -1.6640625  -1.66308594 -1.66308594 -1.6640625\n",
      " -1.66308594 -1.6640625  -1.66308594 -1.6640625  -1.66308594 -1.66308594\n",
      " -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.6640625\n",
      " -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.66308594 -1.6640625\n",
      " -1.6640625  -1.66308594 -1.6640625  -1.66308594 -1.6640625  -1.6640625\n",
      " -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.6640625\n",
      " -1.6640625  -1.6640625  -1.6640625  -1.66308594 -1.66308594 -1.6640625\n",
      " -1.6640625  -1.6640625  -1.66308594 -1.6640625  -1.66308594 -1.6640625\n",
      " -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.6640625\n",
      " -1.66308594 -1.6640625  -1.66308594 -1.6640625  -1.6640625  -1.6640625\n",
      " -1.66308594 -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.66308594\n",
      " -1.66308594 -1.66308594 -1.6640625  -1.6640625  -1.66308594 -1.6640625\n",
      " -1.6640625  -1.66308594 -1.6640625 ]\n",
      "[-1.66181641 -1.66259766 -1.66152344 -1.66064453 -1.66269531 -1.66328125\n",
      " -1.66054688 -1.66347656 -1.66269531 -1.66103516 -1.66367187 -1.66279297\n",
      " -1.66347656 -1.6640625  -1.66357422 -1.6625     -1.66308594 -1.66289062\n",
      " -1.66386719 -1.66347656 -1.66230469 -1.66308594 -1.66230469 -1.6609375\n",
      " -1.66171875 -1.6625     -1.6640625  -1.66152344 -1.66259766 -1.66210938\n",
      " -1.66230469 -1.66269531 -1.6640625  -1.66357422 -1.66367187 -1.6640625\n",
      " -1.66162109 -1.66347656 -1.66289062 -1.66308594 -1.66308594 -1.66269531\n",
      " -1.66367187 -1.6625     -1.6640625  -1.66162109 -1.66347656 -1.66152344\n",
      " -1.66328125 -1.66289062 -1.6640625  -1.66367187 -1.6640625  -1.6640625\n",
      " -1.66269531 -1.66191406 -1.66347656 -1.6640625  -1.66357422 -1.66269531\n",
      " -1.66289062 -1.66269531 -1.66191406 -1.6640625  -1.66269531 -1.66132813\n",
      " -1.66113281 -1.6625     -1.66074219 -1.66054688 -1.66328125 -1.66386719\n",
      " -1.66230469 -1.66347656 -1.6640625  -1.66328125 -1.66132813 -1.66259766\n",
      " -1.66386719 -1.66328125 -1.66113281 -1.66367187 -1.6640625  -1.6625\n",
      " -1.66386719 -1.6640625  -1.6640625  -1.6640625  -1.66357422 -1.66367187\n",
      " -1.65800781 -1.66230469 -1.66347656 -1.6640625  -1.6640625  -1.66289062\n",
      " -1.66152344 -1.66328125 -1.6640625  -1.6640625  -1.6640625  -1.6640625\n",
      " -1.6640625  -1.66210938 -1.6640625  -1.6640625  -1.66367187 -1.66054688\n",
      " -1.66289062 -1.66308594 -1.66289062 -1.6640625  -1.66113281 -1.66289062\n",
      " -1.6640625  -1.6640625  -1.66269531 -1.66230469 -1.66132813 -1.6640625\n",
      " -1.66015625 -1.66289062 -1.65859375 -1.66269531 -1.66347656 -1.66015625\n",
      " -1.66132813 -1.6640625  -1.6640625  -1.66367187 -1.6640625  -1.6640625\n",
      " -1.6640625  -1.66386719 -1.66269531 -1.66152344 -1.66269531 -1.6625\n",
      " -1.66171875 -1.6640625  -1.66386719 -1.6640625  -1.66230469 -1.66347656\n",
      " -1.6640625  -1.66308594 -1.6640625  -1.6640625  -1.6640625  -1.66289062\n",
      " -1.66347656 -1.6625     -1.66328125 -1.66191406 -1.66171875 -1.6625\n",
      " -1.6640625  -1.66289062 -1.66308594 -1.6640625  -1.6640625  -1.6625\n",
      " -1.66308594 -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.6640625\n",
      " -1.66123047 -1.6640625  -1.6640625  -1.66289062 -1.66210938 -1.66289062\n",
      " -1.66396484 -1.66201172 -1.66220703 -1.6640625  -1.66357422 -1.66220703\n",
      " -1.66328125 -1.65996094 -1.6640625  -1.6640625  -1.66328125 -1.6640625\n",
      " -1.6640625  -1.66210938 -1.6640625  -1.65996094 -1.66220703 -1.66259766\n",
      " -1.66318359 -1.66396484 -1.66386719 -1.66298828 -1.6640625  -1.6640625\n",
      " -1.6640625  -1.66386719 -1.66386719 -1.6640625  -1.6640625  -1.66113281\n",
      " -1.66318359 -1.6640625  -1.66269531 -1.6640625  -1.66035156 -1.66210938\n",
      " -1.66308594 -1.6640625  -1.66220703 -1.6640625  -1.6640625  -1.66210938\n",
      " -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.66210938 -1.66230469\n",
      " -1.66337891 -1.66396484 -1.6640625  -1.6640625  -1.66396484 -1.66396484\n",
      " -1.66074219 -1.66328125 -1.66386719 -1.6640625  -1.6609375  -1.6625\n",
      " -1.66328125 -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.6640625\n",
      " -1.66152344 -1.6640625  -1.66308594 -1.66396484 -1.66308594 -1.66308594\n",
      " -1.6640625  -1.66259766 -1.66210938 -1.66386719 -1.66347656 -1.66289062\n",
      " -1.6640625  -1.66376953 -1.66298828 -1.6640625  -1.66376953 -1.66376953\n",
      " -1.6640625  -1.66318359 -1.6640625  -1.66054688 -1.66230469 -1.66191406\n",
      " -1.6640625  -1.6640625  -1.66220703 -1.66181641 -1.66230469 -1.6625\n",
      " -1.66328125 -1.66347656 -1.6640625  -1.6640625  -1.6640625  -1.66347656\n",
      " -1.66113281 -1.6640625  -1.66210938 -1.6640625  -1.6640625  -1.65976562\n",
      " -1.66132813 -1.6640625  -1.6640625  -1.66328125 -1.66289062 -1.66132813\n",
      " -1.66015625 -1.6640625  -1.6640625  -1.66308594 -1.66386719 -1.66289062\n",
      " -1.66074219 -1.66289062 -1.6640625 ]\n",
      "[-1.66113281 -1.66210938 -1.66210938 -1.66113281 -1.66308594 -1.66308594\n",
      " -1.66113281 -1.66308594 -1.66210938 -1.66113281 -1.66210938 -1.66210938\n",
      " -1.66210938 -1.66308594 -1.66308594 -1.66308594 -1.66113281 -1.66308594\n",
      " -1.66308594 -1.66308594 -1.66210938 -1.66308594 -1.66210938 -1.66308594\n",
      " -1.66210938 -1.66210938 -1.66308594 -1.66113281 -1.66308594 -1.66210938\n",
      " -1.66308594 -1.66308594 -1.66308594 -1.66210938 -1.66308594 -1.66308594\n",
      " -1.66210938 -1.66210938 -1.66210938 -1.66210938 -1.66210938 -1.66308594\n",
      " -1.66308594 -1.66308594 -1.66308594 -1.66210938 -1.66308594 -1.66210938\n",
      " -1.66308594 -1.66113281 -1.66308594 -1.66308594 -1.66308594 -1.66308594\n",
      " -1.66308594 -1.66210938 -1.66210938 -1.66308594 -1.66113281 -1.66308594\n",
      " -1.66210938 -1.66113281 -1.66210938 -1.66308594 -1.66210938 -1.66210938\n",
      " -1.66210938 -1.66308594 -1.66113281 -1.66210938 -1.66308594 -1.66210938\n",
      " -1.66210938 -1.66308594 -1.66308594 -1.66308594 -1.66210938 -1.66308594\n",
      " -1.66210938 -1.66308594 -1.66210938 -1.66210938 -1.66308594 -1.66210938\n",
      " -1.66308594 -1.66308594 -1.66308594 -1.66308594 -1.66308594 -1.66210938\n",
      " -1.66113281 -1.66210938 -1.66210938 -1.66308594 -1.66308594 -1.66210938\n",
      " -1.66210938 -1.66308594 -1.66308594 -1.66308594 -1.66308594 -1.66308594\n",
      " -1.66308594 -1.66210938 -1.66308594 -1.66308594 -1.66210938 -1.66210938\n",
      " -1.66210938 -1.66210938 -1.66210938 -1.66210938 -1.66210938 -1.66210938\n",
      " -1.66210938 -1.66308594 -1.66308594 -1.66308594 -1.66210938 -1.66308594\n",
      " -1.66210938 -1.66210938 -1.66113281 -1.66210938 -1.66210938 -1.66113281\n",
      " -1.66210938 -1.66308594 -1.66308594 -1.66210938 -1.66308594 -1.66308594\n",
      " -1.66308594 -1.66210938 -1.66210938 -1.66113281 -1.66210938 -1.66210938\n",
      " -1.66210938 -1.66308594 -1.66210938 -1.66308594 -1.66210938 -1.66210938\n",
      " -1.66308594 -1.66210938 -1.66308594 -1.66308594 -1.66308594 -1.66210938\n",
      " -1.66210938 -1.66210938 -1.66210938 -1.66113281 -1.66210938 -1.66308594\n",
      " -1.66308594 -1.66210938 -1.66308594 -1.66308594 -1.66308594 -1.66210938\n",
      " -1.66210938 -1.66308594 -1.66308594 -1.66308594 -1.66308594 -1.66210938\n",
      " -1.66210938 -1.66308594 -1.66210938 -1.66210938 -1.66210938 -1.66210938\n",
      " -1.66308594 -1.66210938 -1.66308594 -1.66308594 -1.66210938 -1.66210938\n",
      " -1.66308594 -1.66113281 -1.66308594 -1.66308594 -1.66113281 -1.66210938\n",
      " -1.66308594 -1.66210938 -1.66308594 -1.66210938 -1.66210938 -1.66210938\n",
      " -1.66210938 -1.66210938 -1.66210938 -1.66308594 -1.66308594 -1.66308594\n",
      " -1.66308594 -1.66308594 -1.66308594 -1.66308594 -1.66210938 -1.66210938\n",
      " -1.66210938 -1.66308594 -1.66210938 -1.66308594 -1.66210938 -1.66210938\n",
      " -1.66210938 -1.66308594 -1.66210938 -1.66308594 -1.66210938 -1.66210938\n",
      " -1.66308594 -1.66308594 -1.66308594 -1.66308594 -1.66210938 -1.66210938\n",
      " -1.66308594 -1.66308594 -1.66308594 -1.66210938 -1.66308594 -1.66210938\n",
      " -1.66210938 -1.66308594 -1.66308594 -1.66308594 -1.66210938 -1.66113281\n",
      " -1.66308594 -1.66210938 -1.66308594 -1.66308594 -1.66308594 -1.66308594\n",
      " -1.66210938 -1.66308594 -1.66210938 -1.66308594 -1.66210938 -1.66308594\n",
      " -1.66308594 -1.66210938 -1.66210938 -1.66210938 -1.66308594 -1.66210938\n",
      " -1.66210938 -1.66308594 -1.66210938 -1.66308594 -1.66308594 -1.66210938\n",
      " -1.66308594 -1.66308594 -1.66308594 -1.66210938 -1.66210938 -1.66210938\n",
      " -1.66308594 -1.66308594 -1.66308594 -1.66308594 -1.66210938 -1.66210938\n",
      " -1.66308594 -1.66210938 -1.66210938 -1.66210938 -1.66308594 -1.66210938\n",
      " -1.66210938 -1.66308594 -1.66210938 -1.66308594 -1.66308594 -1.66113281\n",
      " -1.66210938 -1.66308594 -1.66113281 -1.66210938 -1.66308594 -1.66113281\n",
      " -1.66308594 -1.66210938 -1.66308594 -1.66210938 -1.66210938 -1.66210938\n",
      " -1.66210938 -1.66210938 -1.66210938]\n",
      "[-1.6640625  -1.66113281 -1.66210938 -1.6640625  -1.6640625  -1.6640625\n",
      " -1.66210938 -1.6640625  -1.66308594 -1.6640625  -1.6640625  -1.6640625\n",
      " -1.66308594 -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.6640625\n",
      " -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.66210938\n",
      " -1.66210938 -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.6640625\n",
      " -1.66210938 -1.66210938 -1.6640625  -1.6640625  -1.6640625  -1.6640625\n",
      " -1.6640625  -1.66308594 -1.66308594 -1.6640625  -1.66113281 -1.6640625\n",
      " -1.66210938 -1.6640625  -1.6640625  -1.66308594 -1.6640625  -1.6640625\n",
      " -1.66308594 -1.6640625  -1.66308594 -1.6640625  -1.66308594 -1.6640625\n",
      " -1.66308594 -1.66308594 -1.66308594 -1.6640625  -1.66308594 -1.66308594\n",
      " -1.6640625  -1.6640625  -1.66113281 -1.6640625  -1.66308594 -1.66210938\n",
      " -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.66210938\n",
      " -1.66210938 -1.66210938 -1.66308594 -1.6640625  -1.66308594 -1.66308594\n",
      " -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.6640625\n",
      " -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.6640625\n",
      " -1.66113281 -1.66113281 -1.6640625  -1.6640625  -1.66308594 -1.66308594\n",
      " -1.66210938 -1.66308594 -1.6640625  -1.6640625  -1.6640625  -1.66308594\n",
      " -1.66308594 -1.66113281 -1.6640625  -1.66308594 -1.66308594 -1.66308594\n",
      " -1.6640625  -1.6640625  -1.66308594 -1.6640625  -1.6640625  -1.66308594\n",
      " -1.6640625  -1.6640625  -1.6640625  -1.66113281 -1.66308594 -1.66210938\n",
      " -1.66113281 -1.6640625  -1.6640625  -1.66308594 -1.6640625  -1.66210938\n",
      " -1.66308594 -1.6640625  -1.6640625  -1.6640625  -1.66308594 -1.6640625\n",
      " -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.66308594 -1.6640625\n",
      " -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.6640625\n",
      " -1.6640625  -1.66113281 -1.6640625  -1.6640625  -1.66308594 -1.6640625\n",
      " -1.6640625  -1.6640625  -1.66308594 -1.66308594 -1.66113281 -1.6640625\n",
      " -1.66210938 -1.66210938 -1.66308594 -1.6640625  -1.66113281 -1.6640625\n",
      " -1.6640625  -1.66210938 -1.6640625  -1.66308594 -1.6640625  -1.6640625\n",
      " -1.66308594 -1.6640625  -1.6640625  -1.6640625  -1.66210938 -1.66308594\n",
      " -1.66113281 -1.66308594 -1.66308594 -1.66113281 -1.6640625  -1.66210938\n",
      " -1.66210938 -1.6640625  -1.6640625  -1.66210938 -1.6640625  -1.66113281\n",
      " -1.66308594 -1.66113281 -1.6640625  -1.66113281 -1.66113281 -1.6640625\n",
      " -1.66210938 -1.66308594 -1.6640625  -1.6640625  -1.6640625  -1.6640625\n",
      " -1.6640625  -1.66308594 -1.6640625  -1.6640625  -1.66113281 -1.66210938\n",
      " -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.6640625\n",
      " -1.66210938 -1.66308594 -1.6640625  -1.6640625  -1.6640625  -1.66210938\n",
      " -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.66210938 -1.66210938\n",
      " -1.6640625  -1.6640625  -1.66308594 -1.66308594 -1.66308594 -1.6640625\n",
      " -1.6640625  -1.66113281 -1.66308594 -1.66308594 -1.66210938 -1.6640625\n",
      " -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.6640625\n",
      " -1.66210938 -1.6640625  -1.6640625  -1.66308594 -1.66308594 -1.66210938\n",
      " -1.6640625  -1.6640625  -1.66308594 -1.66308594 -1.6640625  -1.6640625\n",
      " -1.6640625  -1.66210938 -1.6640625  -1.6640625  -1.6640625  -1.66308594\n",
      " -1.66210938 -1.6640625  -1.6640625  -1.66308594 -1.6640625  -1.66308594\n",
      " -1.6640625  -1.6640625  -1.66308594 -1.6640625  -1.66210938 -1.6640625\n",
      " -1.66210938 -1.6640625  -1.66308594 -1.6640625  -1.66308594 -1.6640625\n",
      " -1.66308594 -1.6640625  -1.66308594 -1.6640625  -1.66308594 -1.66113281\n",
      " -1.66210938 -1.6640625  -1.6640625  -1.6640625  -1.6640625  -1.6640625\n",
      " -1.66210938 -1.6640625  -1.6640625  -1.66210938 -1.6640625  -1.6640625\n",
      " -1.66210938 -1.66308594 -1.66308594]\n",
      "[-1.65820312 -1.66113281 -1.65722656 -1.66113281 -1.66113281 -1.66113281\n",
      " -1.66113281 -1.66113281 -1.65722656 -1.65722656 -1.65820312 -1.66113281\n",
      " -1.65820312 -1.65722656 -1.65722656 -1.66113281 -1.65722656 -1.66113281\n",
      " -1.66113281 -1.66113281 -1.66113281 -1.66113281 -1.66113281 -1.65722656\n",
      " -1.65722656 -1.66113281 -1.66113281 -1.66113281 -1.66113281 -1.65722656\n",
      " -1.66113281 -1.65722656 -1.66113281 -1.65722656 -1.66113281 -1.66113281\n",
      " -1.65722656 -1.65820312 -1.65722656 -1.66113281 -1.65722656 -1.65722656\n",
      " -1.66113281 -1.66113281 -1.66113281 -1.65722656 -1.66113281 -1.65722656\n",
      " -1.66113281 -1.66113281 -1.66113281 -1.65722656 -1.66113281 -1.66113281\n",
      " -1.65722656 -1.65722656 -1.65722656 -1.65722656 -1.66113281 -1.66113281\n",
      " -1.65722656 -1.66113281 -1.65722656 -1.66113281 -1.65722656 -1.65722656\n",
      " -1.66113281 -1.65722656 -1.65722656 -1.66113281 -1.66113281 -1.65722656\n",
      " -1.65722656 -1.65820312 -1.66113281 -1.66113281 -1.65722656 -1.66113281\n",
      " -1.66113281 -1.65722656 -1.66113281 -1.66113281 -1.66113281 -1.65722656\n",
      " -1.66113281 -1.66113281 -1.66113281 -1.66113281 -1.66113281 -1.66113281\n",
      " -1.65722656 -1.65722656 -1.66113281 -1.66113281 -1.65722656 -1.65722656\n",
      " -1.65722656 -1.66113281 -1.66113281 -1.66113281 -1.66113281 -1.66113281\n",
      " -1.66113281 -1.65722656 -1.65722656 -1.65722656 -1.65722656 -1.65722656\n",
      " -1.65722656 -1.65722656 -1.66113281 -1.65820312 -1.65722656 -1.65722656\n",
      " -1.65820312 -1.66113281 -1.66113281 -1.65722656 -1.65722656 -1.65722656\n",
      " -1.65722656 -1.66113281 -1.65722656 -1.66113281 -1.66113281 -1.65722656\n",
      " -1.65722656 -1.66113281 -1.66113281 -1.65722656 -1.65722656 -1.66113281\n",
      " -1.66113281 -1.66113281 -1.66113281 -1.65722656 -1.65722656 -1.65722656\n",
      " -1.66113281 -1.66113281 -1.65722656 -1.66113281 -1.65722656 -1.65722656\n",
      " -1.66113281 -1.65722656 -1.66113281 -1.66113281 -1.66113281 -1.65722656\n",
      " -1.66113281 -1.65722656 -1.65722656 -1.66113281 -1.66113281 -1.65722656\n",
      " -1.65722656 -1.65722656 -1.65722656 -1.65722656 -1.66113281 -1.66113281\n",
      " -1.66113281 -1.66113281 -1.65722656 -1.66113281 -1.65722656 -1.66113281\n",
      " -1.65722656 -1.65722656 -1.66113281 -1.66113281 -1.65820312 -1.65722656\n",
      " -1.65722656 -1.65820312 -1.66113281 -1.66113281 -1.65722656 -1.65722656\n",
      " -1.66113281 -1.65722656 -1.66113281 -1.66113281 -1.65722656 -1.65820312\n",
      " -1.65722656 -1.65722656 -1.66113281 -1.65722656 -1.66113281 -1.66113281\n",
      " -1.66113281 -1.66113281 -1.66113281 -1.66113281 -1.66113281 -1.66113281\n",
      " -1.66113281 -1.65722656 -1.65722656 -1.65722656 -1.65722656 -1.65722656\n",
      " -1.65722656 -1.66113281 -1.66113281 -1.66113281 -1.65722656 -1.66113281\n",
      " -1.65722656 -1.66113281 -1.65722656 -1.66113281 -1.66113281 -1.66113281\n",
      " -1.66113281 -1.66113281 -1.66113281 -1.66113281 -1.65722656 -1.66113281\n",
      " -1.66113281 -1.66113281 -1.66113281 -1.66113281 -1.66113281 -1.66113281\n",
      " -1.66113281 -1.66113281 -1.66113281 -1.66113281 -1.66113281 -1.65722656\n",
      " -1.65722656 -1.66113281 -1.66113281 -1.66113281 -1.66113281 -1.66113281\n",
      " -1.66113281 -1.66113281 -1.66113281 -1.65722656 -1.66113281 -1.65722656\n",
      " -1.66113281 -1.65820312 -1.65722656 -1.65722656 -1.66113281 -1.66113281\n",
      " -1.66113281 -1.66113281 -1.66113281 -1.66113281 -1.65722656 -1.66113281\n",
      " -1.66113281 -1.66113281 -1.66113281 -1.66113281 -1.65820312 -1.65820312\n",
      " -1.65722656 -1.66113281 -1.65722656 -1.65820312 -1.65722656 -1.66113281\n",
      " -1.66113281 -1.65722656 -1.66113281 -1.66113281 -1.66113281 -1.66113281\n",
      " -1.65722656 -1.66113281 -1.65722656 -1.66113281 -1.65722656 -1.65820312\n",
      " -1.65820312 -1.65722656 -1.66113281 -1.65722656 -1.66113281 -1.65820312\n",
      " -1.65722656 -1.66113281 -1.66113281 -1.65820312 -1.65722656 -1.65722656\n",
      " -1.65722656 -1.65722656 -1.66113281]\n"
     ]
    }
   ],
   "source": [
    "# Scale the inputs\n",
    "for i in range(len(X[0])):\n",
    "    # Scale column wise\n",
    "    col = X[:, i]\n",
    "    scaledcol = rescale(col)\n",
    "    print(scaledcol)\n",
    "    # Update the column value.\n",
    "    X[:, i] = scaledcol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and testing datasets (80 - 20).\n",
    "nRows = X.shape[0]\n",
    "nTrain = int(round(0.8*nRows)) \n",
    "nTest = nRows - nTrain\n",
    "# Shuffle row numbers\n",
    "rows = np.arange(nRows)\n",
    "np.random.shuffle(rows)\n",
    "trainIndices = rows[:nTrain]\n",
    "testIndices = rows[nTrain:]\n",
    "\n",
    "Xtrain = X[trainIndices, :]\n",
    "ytrain = y[trainIndices]\n",
    "Xtest = X[testIndices, :]\n",
    "ytest = y[testIndices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.288135593220339"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate out of sample error\n",
    "p = Perceptron()\n",
    "scaledError = outSample(p, Xtrain, ytrain, Xtest, ytest)\n",
    "scaledError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above two experiments, it appears that the classifier has done better on the scaled dataset, than on the unscaled dataset. This is indicated by the fact that, the unscaled dataset yeilds an out-of-sample error of around 0.559, but the scaled dataset only yeilds an error value of 0.338. \n",
    "\n",
    "The classifier on the scaled dataset performs better, due to the fact that, by scaling feature values, we ensure that features with large magnitude values do not have too much undue influence on the weight vector. For example, a feature measuring human height would obviously have a larger magnitude than a feature measuring weight. By normalization, we remove this unwanted effect from the features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.c. \n",
    "\n",
    "An implementation of the standardization function has been provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.93618065,  0.69109474, -2.24062879, ...,  2.26414539,\n",
       "        -0.72197605,  0.65587737],\n",
       "       [ 1.3789285 ,  0.69109474,  0.87388018, ...,  0.6437811 ,\n",
       "         2.47842525, -0.89422007],\n",
       "       [ 1.3789285 ,  0.69109474,  0.87388018, ...,  0.6437811 ,\n",
       "         1.41162482,  1.17257652],\n",
       "       ...,\n",
       "       [ 1.48961547,  0.69109474,  0.87388018, ...,  0.6437811 ,\n",
       "         1.41162482,  1.17257652],\n",
       "       [ 0.27205887,  0.69109474,  0.87388018, ...,  0.6437811 ,\n",
       "         0.34482438,  1.17257652],\n",
       "       [ 0.27205887, -1.44697961, -1.20245913, ...,  0.6437811 ,\n",
       "         0.34482438, -0.89422007]])"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def standardize(X):\n",
    "    for i in range(len(X[0])):\n",
    "        col = X[:, i]\n",
    "        # Find mean\n",
    "        mean = np.mean(col)\n",
    "        # Find SD\n",
    "        sd = np.std(col)\n",
    "        # Loop through feature, updating each value.\n",
    "        for j in range(len(X)):\n",
    "            X[j,i] = (X[j,i] - mean)/sd\n",
    "    return X\n",
    "    \n",
    "X = standardize(X)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardization normalizes the data around the mean value, instead of a specified range. The range of the features appear to be significantly reduced, through this process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Report\n",
    "\n",
    "Answer the questions in the cells reserved for that purpose.\n",
    "\n",
    "Mathematical equations should be written as LaTex equations; the assignment contains multiple examples of both inline formulas (such as the one exemplifying the notation for the norm of a vector $||\\mathbf{x}||$ and those that appear on separate lines, e.g.:\n",
    "\n",
    "$$\n",
    "||\\mathbf{x}|| = \\sqrt{\\mathbf{x}^T \\mathbf{x}}.\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### Submission\n",
    "\n",
    "Submit your report as a Jupyter notebook via Canvas.  Running the notebook should generate all the plots and results in your notebook.\n",
    "\n",
    "\n",
    "### Grading \n",
    "\n",
    "Here is what the grade sheet will look like for this assignment.  A few general guidelines for this and future assignments in the course:\n",
    "\n",
    "  * Your answers should be concise and to the point.  We will take off points if that is not the case.\n",
    "  * Always provide a description of the method you used to produce a given result in sufficient detail such that the reader can reproduce your results on the basis of the description.  You can use a few lines of python code or pseudo-code.\n",
    "\n",
    "\n",
    "Grading sheet for the assignment:\n",
    "\n",
    "```\n",
    "Part 1:  60 points.\n",
    "(30 points):  Correct implementation of the classifiers\n",
    "(15 points):  Good protocol for evaluating classifier accuracy; results are provided in a clear and concise way\n",
    "(15 points):  Discussion of the results\n",
    "\n",
    "Part 2:  20 points.\n",
    "(15 points):  Learning curves are correctly generated and displayed in a clear and readable way\n",
    "( 5 points):  Discussion of the results\n",
    "\n",
    "Part 3:  20 points.\n",
    "( 5 points):  How to perform data scaling\n",
    "(10 points):  Comparison of normalized/raw data results; discussion of results\n",
    "( 5 points):  Range of features after standardization\n",
    "```\n",
    "\n",
    "\n",
    "Grading will be based on the following criteria:\n",
    "\n",
    "  * Correctness of answers to math problems\n",
    "  * Math is formatted as LaTex equations\n",
    "  * Correct behavior of the required code\n",
    "  * Easy to understand plots \n",
    "  * Overall readability and organization of the notebook\n",
    "  * Effort in making interesting observations where requested.\n",
    "  * Conciseness.  Points may be taken off if the notebook is overly \n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
